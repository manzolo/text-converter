# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
MAX_FILE_SIZE=104857600  # 100MB in bytes
CHUNK_SIZE=1048576  # 1MB chunks

# Ollama Configuration
OLLAMA_HOST=http://ollama:11434  # Use 'http://ollama:11434' for local docker
OLLAMA_MODEL=llama3.1:8b  # Available models: llama3.1, mistral, codellama, phi3, etc.

# GPU Support (true or false)
USE_GPU=false

# Use external Ollama instance (true or false)
USE_EXTERNAL_OLLAMA=false
# Use 'http://host.docker.internal:11434' to connect to Ollama on host machine
# Or use specific IP like 'http://192.168.1.100:11434' for remote Ollama
EXTERNAL_OLLAMA_HOST=http://host.docker.internal:11434
